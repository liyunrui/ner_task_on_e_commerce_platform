{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example to demonstrate tf_idf\n",
    "### Reference:https://medium.freecodecamp.org/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bow):\n",
    "    '''\n",
    "    compute term freq given the word occurence in self document \n",
    "    so TF it will change as the item_name changes even the word is the same.\n",
    "    '''\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict\n",
    "\n",
    "def computeIDF(docList, smoothing = False):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList) # total number of item_name\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        # val: number of item_name containing these words.\n",
    "        if smoothing == True:\n",
    "            idfDict[word] = math.log10(1.0 + N / float(val))\n",
    "        else:\n",
    "            idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict # word of iverse document freq won't is fixed given all sentence\n",
    "\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        # val: term freq\n",
    "        # idfs\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "#-------------\n",
    "# data\n",
    "#-------------\n",
    "docA = \"The cat sat on my face\"\n",
    "docB = \"The dog sat on my bed\"\n",
    "bowA = docA.split(\" \")\n",
    "bowB = docB.split(\" \")\n",
    "print ('bowA', bowA)\n",
    "print ('bowB', bowB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary\n",
    "wordSet = set(bowA).union(set(bowB))\n",
    "\n",
    "# The method fromkeys() creates a new dictionary with keys from seq and values set to value.\n",
    "\n",
    "# initalize \n",
    "wordDictA = dict.fromkeys(wordSet, 0) \n",
    "wordDictB = dict.fromkeys(wordSet, 0)\n",
    "print ('wordDictA', wordDictA)\n",
    "print ('wordDictB', wordDictB)\n",
    "\n",
    "# corpust states computation\n",
    "for word in bowA:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in bowB:\n",
    "    wordDictB[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_dataframe\n",
    "pd.DataFrame([wordDictA, wordDictB])\n",
    "# compute tf\n",
    "tfBowA = computeTF(wordDictA, bowA)\n",
    "tfBowB = computeTF(wordDictB, bowB)\n",
    "df = pd.DataFrame([tfBowA, tfBowB], index = ['s1', 's2'])\n",
    "df = df.add_suffix('_TF')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute idf\n",
    "idfs = computeIDF([wordDictA, wordDictB], smoothing = True)\n",
    "tfidfBowA = computeTFIDF(tfBowA, idfs)\n",
    "tfidfBowB = computeTFIDF(tfBowB, idfs)\n",
    "df = pd.DataFrame([tfidfBowA, tfidfBowB])\n",
    "df = df.add_suffix('_TFIDF')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn\n",
    "# Note : tf-idfs computed in scikit-learnâ€™s TfidfTransformer and TfidfVectorizer differ slightly from the standard textbook notation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "'''\n",
    "Input: String which is not suitable for us since our token is cleaned.\n",
    "\n",
    "'''\n",
    "tfidf = TfidfVectorizer()\n",
    "response = tfidf.fit_transform([docA, docB]) # parse matrix \n",
    "vocabulary = tfidf.get_feature_names() # v\n",
    "print ('vocabulary', vocabulary)\n",
    "for col in response.nonzero()[1]:\n",
    "    print (feature_names[col], ' - ', response[0, col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfTransformer\n",
    "# http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=False, sublinear_tf=False,\n",
       "         use_idf=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf = False)\n",
    "transformer   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [[3, 0, 1],\n",
    "          [2, 0, 0],\n",
    "          [3, 0, 0],\n",
    "          [4, 0, 0],\n",
    "          [3, 2, 0],\n",
    "          [3, 0, 2]]\n",
    "\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "tfidf                         \n",
    "\n",
    "\n",
    "\n",
    "tfidf.toarray()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# real_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_data_path\n",
    "input_base_path = '../brand_recognition_bio/data/preprocessed'\n",
    "T = 3\n",
    "#--------------------\n",
    "# laod data including label\n",
    "#--------------------\n",
    "if T == 1:\n",
    "    name = 'tv_and_laptop' \n",
    "    df = pd.read_csv(os.path.join(input_base_path, 'tv_and_laptop.csv'))\n",
    "elif T == 2:\n",
    "    name = 'personal_care_and_beauty'\n",
    "    df = pd.read_csv(os.path.join(input_base_path, 'personal_care_and_beauty.csv'))\n",
    "elif T == 3:\n",
    "    name = 'beauty_amazon'\n",
    "    df = pd.read_csv(os.path.join(input_base_path, 'beauty_amazon.csv')) # 40649 x 87402\n",
    "elif T == 4:\n",
    "    name = 'tv_laptop_amazon'\n",
    "    df = pd.read_csv(os.path.join(input_base_path, 'tv_laptop_amazon.csv')) # 16103 x 8324\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24146133741941755"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16103 /5 * 8324 / (40649/32 * 87402)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40649\n"
     ]
    }
   ],
   "source": [
    "#-------------------------\n",
    "# drop itemname and tokens with nan\n",
    "#-------------------------\n",
    "df.dropna(subset = ['item_name', 'tokens'], axis = 0, inplace = True)\n",
    "#--------------------------\n",
    "# conver type\n",
    "#--------------------------\n",
    "df['tokens'] = df.tokens.astype(str)\n",
    "#--------------------------\n",
    "# preprocessing\n",
    "#--------------------------\n",
    "\n",
    "df['tokens'] = df.tokens.apply(lambda x: x.lower() if type(x)==str else x)\n",
    "wordSet = df.tokens.unique()\n",
    "print (len(wordSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8324"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.item_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_name_ls = list(df.item_name.unique())\n",
    "item_name_num = df.item_name.nunique()\n",
    "num_partitions = 10\n",
    "n = int(item_name_num /num_partitions)\n",
    "n * 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from billiard import Pool\n",
    "\n",
    "def parallelize_dataframe(df, func, name):\n",
    "    '''\n",
    "    speeding up DataFrame.apply() via parallelizing.\n",
    "\n",
    "    '''\n",
    "    if name == 'beauty_amazon':\n",
    "        #---------------\n",
    "        # setting\n",
    "        #---------------\n",
    "        num_partitions = 32\n",
    "        num_cores = 32\n",
    "\n",
    "        # core\n",
    "        item_name_ls = list(df.item_name.unique())\n",
    "        item_name_num = df.item_name.nunique()\n",
    "        n = int(item_name_num /num_partitions)\n",
    "        # split df based on item_name\n",
    "        df1 = df[df.item_name.isin(item_name_ls[:1*n])]\n",
    "        df2 = df[df.item_name.isin(item_name_ls[1*n:2*n])]\n",
    "        df3 = df[df.item_name.isin(item_name_ls[2*n:3*n])]\n",
    "        df4 = df[df.item_name.isin(item_name_ls[3*n:4*n])]\n",
    "        df5 = df[df.item_name.isin(item_name_ls[4*n:5*n])]\n",
    "        df6 = df[df.item_name.isin(item_name_ls[5*n:6*n])]\n",
    "        df7 = df[df.item_name.isin(item_name_ls[6*n:7*n])]\n",
    "        df8 = df[df.item_name.isin(item_name_ls[7*n:8*n])]\n",
    "        df9 = df[df.item_name.isin(item_name_ls[8*n:9*n])]\n",
    "        df10 = df[df.item_name.isin(item_name_ls[9*n:10*n])]\n",
    "        df11 = df[df.item_name.isin(item_name_ls[10*n:11*n])]\n",
    "        df12 = df[df.item_name.isin(item_name_ls[11*n:12*n])]\n",
    "        df13 = df[df.item_name.isin(item_name_ls[12*n:13*n])]\n",
    "        df14 = df[df.item_name.isin(item_name_ls[13*n:14*n])]\n",
    "        df15 = df[df.item_name.isin(item_name_ls[14*n:15*n])]\n",
    "        df16 = df[df.item_name.isin(item_name_ls[15*n:16*n])]\n",
    "        df17 = df[df.item_name.isin(item_name_ls[16*n:17*n])]\n",
    "        df18 = df[df.item_name.isin(item_name_ls[17*n:18*n])]\n",
    "        df19 = df[df.item_name.isin(item_name_ls[18*n:19*n])]\n",
    "        df20 = df[df.item_name.isin(item_name_ls[19*n:20*n])]\n",
    "        df21 = df[df.item_name.isin(item_name_ls[20*n:21*n])]\n",
    "        df22 = df[df.item_name.isin(item_name_ls[21*n:22*n])]\n",
    "        df23 = df[df.item_name.isin(item_name_ls[22*n:23*n])]\n",
    "        df24 = df[df.item_name.isin(item_name_ls[23*n:24*n])]\n",
    "        df25 = df[df.item_name.isin(item_name_ls[24*n:25*n])]\n",
    "        df26 = df[df.item_name.isin(item_name_ls[25*n:26*n])]\n",
    "        df27 = df[df.item_name.isin(item_name_ls[26*n:27*n])]\n",
    "        df28 = df[df.item_name.isin(item_name_ls[27*n:28*n])]\n",
    "        df29 = df[df.item_name.isin(item_name_ls[28*n:29*n])]\n",
    "        df30 = df[df.item_name.isin(item_name_ls[29*n:30*n])]\n",
    "        df31 = df[df.item_name.isin(item_name_ls[30*n:31*n])]\n",
    "        df32 = df[df.item_name.isin(item_name_ls[31*n:])]\n",
    "\n",
    "        pool = Pool(num_cores)\n",
    "        df = pd.concat(pool.map(func, [df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,\n",
    "                                       df11,df12,df13,df14,df15,df16,df17,df18,df19,df20,\n",
    "                                       df21,df22,df23,df24,df25,df26,df27,df28,df29,df30,\n",
    "                                       df31,df32]))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    \t\n",
    "    else:\n",
    "        #---------------\n",
    "        # setting\n",
    "        #---------------\n",
    "        num_partitions = 5\n",
    "        num_cores = 5\n",
    "\n",
    "        # core\n",
    "        item_name_ls = list(df.item_name.unique())\n",
    "        item_name_num = df.item_name.nunique()\n",
    "        n = int(item_name_num /num_partitions)\n",
    "        # split df based on item_name\n",
    "        df1 = df[df.item_name.isin(item_name_ls[:n])]\n",
    "        df2 = df[df.item_name.isin(item_name_ls[n:2*n])]\n",
    "        df3 = df[df.item_name.isin(item_name_ls[2*n:3*n])]\n",
    "        df4 = df[df.item_name.isin(item_name_ls[3*n:4*n])]\n",
    "        df5 = df[df.item_name.isin(item_name_ls[4*n:])]\n",
    "        pool = Pool(num_cores)\n",
    "        df = pd.concat(pool.map(func, [df1,df2,df3,df4,df5]))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    return df\n",
    "\n",
    "def speed_up_func_for_feature_engineering(df):\n",
    "    '''\n",
    "    Put the columns u need to apply()\n",
    "    \n",
    "    data: DataFrame\n",
    "    '''\n",
    "    df = df.groupby('item_name').apply(lambda x: get_count_metrix(x, wordSet)).reset_index()\n",
    "    return df\n",
    "\n",
    "def get_count_metrix(df, wordSet):\n",
    "    '''\n",
    "    return the matrix, row is number of item_names, column is number of words aka size of vocabulary.\n",
    "    Note:\n",
    "        the element in matrix is called term frequency given the item title.\n",
    "    args:\n",
    "    ---------\n",
    "    df: DataFrame\n",
    "    wordSet: set\n",
    "    '''\n",
    "    # initalize empty dict\n",
    "    wordDictA = dict.fromkeys(wordSet, 0) \n",
    "    # corpust states computation\n",
    "    for word in df.tokens.tolist():\n",
    "        wordDictA[word]+=1\n",
    "    return pd.DataFrame([wordDictA])\n",
    "\n",
    "s = time.time()\n",
    "#df_count_ = df.head(100).groupby('item_name').apply(lambda x: get_count_metrix(x, wordSet)).reset_index()\n",
    "df_count = parallelize_dataframe(df, speed_up_func_for_feature_engineering, name)\n",
    "e = time.time()\n",
    "\n",
    "print (e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count.reset_index(inplace=True, drop=True)\n",
    "counts = df_count.values[:,1:]\n",
    "tfidf = transformer.fit_transform(counts.tolist())\n",
    "# output\n",
    "tf_idf_df = pd.DataFrame(tfidf.toarray())\n",
    "tf_idf_df.columns = df_count.columns.tolist()[1:]\n",
    "output = pd.concat([df_count[['item_name']], tf_idf_df], axis = 1).set_index('item_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = []\n",
    "for ix, row in df.iterrows():\n",
    "    i_n = row.item_name\n",
    "    t = row.tokens\n",
    "    tf_idf.append(output.loc[i_n,t])# loc: for index which is name not int\n",
    "df['tf_idf'] = tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    }
   ],
   "source": [
    "file_path = '../brand_detector/features/tv_and_laptop/tf_idf.h5'\n",
    "if os.path.exists(file_path) == True:\n",
    "    print ('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
