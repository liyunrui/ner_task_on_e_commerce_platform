{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example to demonstrate tf_idf\n",
    "### Reference:https://medium.freecodecamp.org/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bow):\n",
    "    '''\n",
    "    compute term freq given the word occurence in self document \n",
    "    so TF it will change as the item_name changes even the word is the same.\n",
    "    '''\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict\n",
    "\n",
    "def computeIDF(docList, smoothing = False):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList) # total number of item_name\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        # val: number of item_name containing these words.\n",
    "        if smoothing == True:\n",
    "            idfDict[word] = math.log10(1.0 + N / float(val))\n",
    "        else:\n",
    "            idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict # word of iverse document freq won't is fixed given all sentence\n",
    "\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        # val: term freq\n",
    "        # idfs\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "#-------------\n",
    "# data\n",
    "#-------------\n",
    "docA = \"The cat sat on my face\"\n",
    "docB = \"The dog sat on my bed\"\n",
    "bowA = docA.split(\" \")\n",
    "bowB = docB.split(\" \")\n",
    "print ('bowA', bowA)\n",
    "print ('bowB', bowB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary\n",
    "wordSet = set(bowA).union(set(bowB))\n",
    "\n",
    "# The method fromkeys() creates a new dictionary with keys from seq and values set to value.\n",
    "\n",
    "# initalize \n",
    "wordDictA = dict.fromkeys(wordSet, 0) \n",
    "wordDictB = dict.fromkeys(wordSet, 0)\n",
    "print ('wordDictA', wordDictA)\n",
    "print ('wordDictB', wordDictB)\n",
    "\n",
    "# corpust states computation\n",
    "for word in bowA:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in bowB:\n",
    "    wordDictB[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_dataframe\n",
    "pd.DataFrame([wordDictA, wordDictB])\n",
    "# compute tf\n",
    "tfBowA = computeTF(wordDictA, bowA)\n",
    "tfBowB = computeTF(wordDictB, bowB)\n",
    "df = pd.DataFrame([tfBowA, tfBowB], index = ['s1', 's2'])\n",
    "df = df.add_suffix('_TF')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute idf\n",
    "idfs = computeIDF([wordDictA, wordDictB], smoothing = True)\n",
    "tfidfBowA = computeTFIDF(tfBowA, idfs)\n",
    "tfidfBowB = computeTFIDF(tfBowB, idfs)\n",
    "df = pd.DataFrame([tfidfBowA, tfidfBowB])\n",
    "df = df.add_suffix('_TFIDF')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn\n",
    "# Note : tf-idfs computed in scikit-learnâ€™s TfidfTransformer and TfidfVectorizer differ slightly from the standard textbook notation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "'''\n",
    "Input: String which is not suitable for us since our token is cleaned.\n",
    "\n",
    "'''\n",
    "tfidf = TfidfVectorizer()\n",
    "response = tfidf.fit_transform([docA, docB]) # parse matrix \n",
    "vocabulary = tfidf.get_feature_names() # v\n",
    "print ('vocabulary', vocabulary)\n",
    "for col in response.nonzero()[1]:\n",
    "    print (feature_names[col], ' - ', response[0, col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfTransformer\n",
    "# http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf = False)\n",
    "transformer   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [[3, 0, 1],\n",
    "          [2, 0, 0],\n",
    "          [3, 0, 0],\n",
    "          [4, 0, 0],\n",
    "          [3, 2, 0],\n",
    "          [3, 0, 2]]\n",
    "\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "tfidf                         \n",
    "\n",
    "\n",
    "\n",
    "tfidf.toarray()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# real_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preprocessed_data_path\n",
    "input_base_path = '../brand_detector/data/preprocessed'\n",
    "T = 1\n",
    "#--------------------\n",
    "# laod data including label\n",
    "#--------------------\n",
    "if T == 1:\n",
    "    name = 'tv_and_laptop' \n",
    "    df = pd.read_csv(os.path.join(input_base_path, 'tv_and_laptop.csv'))\n",
    "elif T == 2:\n",
    "    name = 'personal_care_and_beauty'\n",
    "    df = pd.read_csv(os.path.join(input_base_path, 'personal_care_and_beauty.csv'))\n",
    "elif T == 3:\n",
    "    name = 'beauty_amazon'\n",
    "    df = pd.read_csv(os.path.join(input_base_path, 'beauty_amazon.csv'))\n",
    "elif T == 4:\n",
    "    name = 'tv_laptop_amazon'\n",
    "    df = pd.read_csv(os.path.join(input_base_path, 'tv_laptop_amazon.csv'))\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "# drop itemname and tokens with nan\n",
    "#-------------------------\n",
    "df.dropna(subset = ['item_name', 'tokens'], axis = 0, inplace = True)\n",
    "#--------------------------\n",
    "# conver type\n",
    "#--------------------------\n",
    "df['tokens'] = df.tokens.astype(str)\n",
    "#--------------------------\n",
    "# preprocessing\n",
    "#--------------------------\n",
    "\n",
    "df['tokens'] = df.tokens.apply(lambda x: x.lower() if type(x)==str else x)\n",
    "wordSet = df.tokens.unique()\n",
    "print (len(wordSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_name_ls = list(df.item_name.unique())\n",
    "item_name_num = df.item_name.nunique()\n",
    "num_partitions = 10\n",
    "n = int(item_name_num /num_partitions)\n",
    "n * 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from billiard import Pool\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    '''\n",
    "    speeding up DataFrame.apply() via parallelizing.\n",
    "\n",
    "    '''\n",
    "    #---------------\n",
    "    # setting\n",
    "    #---------------\n",
    "    num_partitions = 5\n",
    "    num_cores = 10\n",
    "\n",
    "    # core\n",
    "    item_name_ls = list(df.item_name.unique())\n",
    "    item_name_num = df.item_name.nunique()\n",
    "    n = int(item_name_num /num_partitions)\n",
    "    # split df based on item_name\n",
    "    df1 = df[df.item_name.isin(item_name_ls[:n])]\n",
    "    df2 = df[df.item_name.isin(item_name_ls[n:2*n])]\n",
    "    df3 = df[df.item_name.isin(item_name_ls[2*n:3*n])]\n",
    "    df4 = df[df.item_name.isin(item_name_ls[3*n:4*n])]\n",
    "    df5 = df[df.item_name.isin(item_name_ls[4*n:])]\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, [df1,df2,df3,df4,df5]))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def speed_up_func_for_feature_engineering(df):\n",
    "    '''\n",
    "    Put the columns u need to apply()\n",
    "    \n",
    "    data: DataFrame\n",
    "    '''\n",
    "    df = df.groupby('item_name').apply(lambda x: get_count_metrix(x, wordSet)).reset_index()\n",
    "    return df\n",
    "\n",
    "def get_count_metrix(df, wordSet):\n",
    "    '''\n",
    "    return the matrix, row is number of item_names, column is number of words aka size of vocabulary.\n",
    "    Note:\n",
    "        the element in matrix is called term frequency given the item title.\n",
    "    args:\n",
    "    ---------\n",
    "    df: DataFrame\n",
    "    wordSet: set\n",
    "    '''\n",
    "    # initalize empty dict\n",
    "    wordDictA = dict.fromkeys(wordSet, 0) \n",
    "    # corpust states computation\n",
    "    for word in df.tokens.tolist():\n",
    "        wordDictA[word]+=1\n",
    "    return pd.DataFrame([wordDictA])\n",
    "\n",
    "s = time.time()\n",
    "#df_count_ = df.head(100).groupby('item_name').apply(lambda x: get_count_metrix(x, wordSet)).reset_index()\n",
    "df_count = parallelize_dataframe(df, speed_up_func_for_feature_engineering)\n",
    "e = time.time()\n",
    "\n",
    "print (e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count.reset_index(inplace=True, drop=True)\n",
    "counts = df_count.values[:,1:]\n",
    "tfidf = transformer.fit_transform(counts.tolist())\n",
    "# output\n",
    "tf_idf_df = pd.DataFrame(tfidf.toarray())\n",
    "tf_idf_df.columns = df_count.columns.tolist()[1:]\n",
    "output = pd.concat([df_count[['item_name']], tf_idf_df], axis = 1).set_index('item_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = []\n",
    "for ix, row in df.iterrows():\n",
    "    i_n = row.item_name\n",
    "    t = row.tokens\n",
    "    tf_idf.append(output.loc[i_n,t])# loc: for index which is name not int\n",
    "df['tf_idf'] = tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
