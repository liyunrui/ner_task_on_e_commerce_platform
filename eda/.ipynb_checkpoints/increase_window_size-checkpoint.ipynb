{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "\"\"\"\n",
    "Created on Aug 21 2018\n",
    "\n",
    "In order to capture similarity between tokens and surrounding tokens.\n",
    "@author: Ray\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import nltk, re, string, collections\n",
    "from nltk.util import ngrams # function for making ngrams\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime # for the newest version control\n",
    "import os\n",
    "import time\n",
    "import multiprocessing as mp # for speeding up some process\n",
    "import logging\n",
    "from nltk import tag # for pos_tagging\n",
    "from nltk.corpus import wordnet # for geting pos of wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gc\n",
    "import multiprocessing # for parallelling apply() in panda\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def get_the_preceding_word(row, window_size = 1):\n",
    "    '''\n",
    "    Get the preceding word given the token. \n",
    "    It's a helper function to compute the sequential feature of the word.\n",
    "    '''\n",
    "    try:\n",
    "        the_former_ix = row.item_name.split().index(row.tokens) - window_size\n",
    "        if the_former_ix < 0:\n",
    "            return -1 # It means the former word is non-existent. # -1 is bettern than missing value\n",
    "        else:\n",
    "            return row.item_name.split()[the_former_ix]\n",
    "    except Exception:\n",
    "        pass # It will make missing value on this feature but it's fine\n",
    "\n",
    "def get_the_succeeding_word(row, window_size = 1):\n",
    "    '''\n",
    "    Get the succeeding word given the token. \n",
    "    It's a helper function to compute the sequential feature of the word.\n",
    "    '''\n",
    "    try:\n",
    "        the_latter_ix = row.item_name.split().index(row.tokens) + window_size\n",
    "        if the_latter_ix >= len(row.item_name.split()):\n",
    "            return -1 # It means the latter word is non-existent. \n",
    "        else:\n",
    "            return row.item_name.split()[the_latter_ix]\n",
    "    except Exception:\n",
    "        pass # It will make missing value on this feature but it's fine\n",
    "\n",
    "def succeeding_2_gram_given_current_token(row, esBigramFreq):\n",
    "    if row.the_succeeding_word_given_current_token_w_1 == -1:\n",
    "        return -1\n",
    "    else:\n",
    "        key = (row.tokens.lower(), row.the_succeeding_word_given_current_token_w_1.lower())\n",
    "        return esBigramFreq[key]\n",
    "    return row\n",
    "\n",
    "def preceding_2_gram_given_current_token(row, esBigramFreq):\n",
    "    if row.the_preceding_word_given_current_token_w_1 == -1:\n",
    "        return -1\n",
    "    else:\n",
    "        key = (row.tokens.lower(), row.the_preceding_word_given_current_token_w_1.lower())\n",
    "        return esBigramFreq[key]\n",
    "    return row\n",
    "\n",
    "def preceding_3_gram_given_current_token(row, esTrigramFreq):\n",
    "    if row.the_preceding_word_given_current_token_w_1 != -1 and row.the_preceding_word_given_current_token_w_2 != -1:\n",
    "        key = (row.tokens.lower(), row.the_preceding_word_given_current_token_w_1.lower(),\n",
    "               row.the_preceding_word_given_current_token_w_2.lower())\n",
    "        return esTrigramFreq[key]\n",
    "    else:\n",
    "        return -1\n",
    "    return row\n",
    "\n",
    "def succeeding_3_gram_given_current_token(row, esTrigramFreq):\n",
    "    if row.the_succeeding_word_given_current_token_w_1 != -1 and row.the_succeeding_word_given_current_token_w_2 != -1:\n",
    "        key = (row.tokens.lower(), row.the_succeeding_word_given_current_token_w_1.lower(),\n",
    "               row.the_succeeding_word_given_current_token_w_2.lower())\n",
    "        return esTrigramFreq[key]\n",
    "    else:\n",
    "        return -1\n",
    "    return row\n",
    "\n",
    "def preceding_4_gram_given_current_token(row, esFgramFreq):\n",
    "    if (row.the_preceding_word_given_current_token_w_1 != -1) \\\n",
    "    and (row.the_preceding_word_given_current_token_w_2 != -1) \\\n",
    "    and (row.the_preceding_word_given_current_token_w_3 != -1):\n",
    "        key = (row.tokens.lower(), \n",
    "               row.the_preceding_word_given_current_token_w_1.lower(),\n",
    "               row.the_preceding_word_given_current_token_w_2.lower(),\n",
    "               row.the_preceding_word_given_current_token_w_3.lower())\n",
    "        return esFgramFreq[key]\n",
    "    else:\n",
    "        return -1\n",
    "    return row\n",
    "\n",
    "def succeeding_4_gram_given_current_token(row, esFgramFreq):\n",
    "    if (row.the_succeeding_word_given_current_token_w_1 != -1) \\\n",
    "    and (row.the_succeeding_word_given_current_token_w_2 != -1) \\\n",
    "    and (row.the_succeeding_word_given_current_token_w_3 != -1):\n",
    "        key = (row.tokens.lower(), \n",
    "               row.the_succeeding_word_given_current_token_w_1.lower(),\n",
    "               row.the_succeeding_word_given_current_token_w_2.lower(),\n",
    "               row.the_succeeding_word_given_current_token_w_3.lower())\n",
    "        return esFgramFreq[key]\n",
    "    else:\n",
    "        return -1\n",
    "    return row\n",
    "\n",
    "def preceding_5_gram_given_current_token(row, esFivegramFreq):\n",
    "    if (row.the_preceding_word_given_current_token_w_1 != -1) \\\n",
    "    and (row.the_preceding_word_given_current_token_w_2 != -1) \\\n",
    "    and (row.the_preceding_word_given_current_token_w_3 != -1) \\\n",
    "    and (row.the_preceding_word_given_current_token_w_4 != -1):\n",
    "        key = (row.tokens.lower(), \n",
    "               row.the_preceding_word_given_current_token_w_1.lower(),\n",
    "               row.the_preceding_word_given_current_token_w_2.lower(),\n",
    "               row.the_preceding_word_given_current_token_w_3.lower(),\n",
    "               row.the_preceding_word_given_current_token_w_4.lower())\n",
    "        return esFivegramFreq[key]\n",
    "    else:\n",
    "        return -1\n",
    "    return row\n",
    "\n",
    "def succeeding_5_gram_given_current_token(row, esFivegramFreq):\n",
    "    if (row.the_succeeding_word_given_current_token_w_1 != -1) \\\n",
    "    and (row.the_succeeding_word_given_current_token_w_2 != -1) \\\n",
    "    and (row.the_succeeding_word_given_current_token_w_3 != -1) \\\n",
    "    and (row.the_succeeding_word_given_current_token_w_4 != -1):\n",
    "        key = (row.tokens.lower(), \n",
    "               row.the_succeeding_word_given_current_token_w_1.lower(),\n",
    "               row.the_succeeding_word_given_current_token_w_2.lower(),\n",
    "               row.the_succeeding_word_given_current_token_w_3.lower(),\n",
    "               row.the_succeeding_word_given_current_token_w_4.lower())\n",
    "        return esFivegramFreq[key]\n",
    "    else:\n",
    "        return -1\n",
    "    return row\n",
    "\n",
    "def succeeding_strip_2_gram_given_current_token(row, esBigramFreq):\n",
    "    if row.the_succeeding_word_given_current_token_w_2 == -1:\n",
    "        return -1\n",
    "    else:\n",
    "        key = (row.tokens.lower(), row.the_succeeding_word_given_current_token_w_2.lower())\n",
    "        return esBigramFreq[key]\n",
    "    return row\n",
    "\n",
    "def preceding_strip_2_gram_given_current_token(row, esBigramFreq):\n",
    "    if row.the_preceding_word_given_current_token_w_2 == -1:\n",
    "        return -1\n",
    "    else:\n",
    "        key = (row.tokens.lower(), row.the_preceding_word_given_current_token_w_2.lower())\n",
    "        return esBigramFreq[key]\n",
    "    return row\n",
    "\n",
    "def succeeding_strip_3_gram_given_current_token(row, esBigramFreq):\n",
    "    if row.the_succeeding_word_given_current_token_w_3 == -1:\n",
    "        return -1\n",
    "    else:\n",
    "        key = (row.tokens.lower(), row.the_succeeding_word_given_current_token_w_3.lower())\n",
    "        return esBigramFreq[key]\n",
    "    return row\n",
    "\n",
    "def preceding_strip_3_gram_given_current_token(row, esBigramFreq):\n",
    "    if row.the_preceding_word_given_current_token_w_3 == -1:\n",
    "        return -1\n",
    "    else:\n",
    "        key = (row.tokens.lower(), row.the_preceding_word_given_current_token_w_3.lower())\n",
    "        return esBigramFreq[key]\n",
    "    return row\n",
    "\n",
    "def succeeding_strip_4_gram_given_current_token(row, esBigramFreq):\n",
    "    if row.the_succeeding_word_given_current_token_w_4 == -1:\n",
    "        return -1\n",
    "    else:\n",
    "        key = (row.tokens.lower(), row.the_succeeding_word_given_current_token_w_4.lower())\n",
    "        return esBigramFreq[key]\n",
    "    return row\n",
    "\n",
    "def preceding_strip_4_gram_given_current_token(row, esBigramFreq):\n",
    "    if row.the_preceding_word_given_current_token_w_4 == -1:\n",
    "        return -1\n",
    "    else:\n",
    "        key = (row.tokens.lower(), row.the_preceding_word_given_current_token_w_4.lower())\n",
    "        return esBigramFreq[key]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_data_path\n",
    "input_base_path = '../brand_detector/data/preprocessed'\n",
    "T = 1\n",
    "#--------------------\n",
    "# laod data including label\n",
    "#--------------------\t\n",
    "if T == 1:\n",
    "    name = 'tv_and_laptop' \n",
    "    df = pd.read_csv(os.path.join(input_base_path, 'tv_and_laptop.csv'))\n",
    "elif T == 2:\n",
    "    name = 'personal_care_and_beauty'\n",
    "    df = pd.read_csv(os.path.join(input_base_path, 'personal_care_and_beauty.csv'))\n",
    "elif T == 3:\n",
    "    name = 'beauty_amazon'\n",
    "    df = pd.read_csv(os.path.join(input_base_path, 'beauty_amazon.csv'))\n",
    "elif T == 4:\n",
    "    name = 'tv_laptop_amazon'\n",
    "    df = pd.read_csv(os.path.join(input_base_path, 'tv_laptop_amazon.csv'))\n",
    "else:\n",
    "    pass\n",
    "tokenized = [t.lower() for t in df.tokens.tolist()]\n",
    "\n",
    "#----------------------------\n",
    "# n-grame generator\n",
    "#----------------------------\n",
    "esBigrams = ngrams(tokenized, 2) # generater\n",
    "esTrigrams = ngrams(tokenized, 3) # generater\n",
    "esFgrams = ngrams(tokenized, 4) # generater\n",
    "esFivegrams = ngrams(tokenized, 5) # generater\n",
    "\n",
    "#----------------------------\n",
    "# get the frequency of each bigram in our corpus\n",
    "#----------------------------\n",
    "esBigramFreq = collections.Counter(esBigrams)\n",
    "esTrigramFreq = collections.Counter(esTrigrams)\n",
    "esFgramFreq = collections.Counter(esFgrams)\n",
    "esFivegramFreq = collections.Counter(esFivegrams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------\n",
    "# drop itemname and tokens with nan\n",
    "#-------------------------\n",
    "df.dropna(subset = ['item_name', 'tokens'], axis = 0, inplace = True)\n",
    "#--------------------------\n",
    "# conver type\n",
    "#--------------------------\n",
    "df['tokens'] = df.tokens.astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wo swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #--------------------------\n",
    "# # preprocessing for contextual information\n",
    "# #--------------------------\n",
    "# s = time.time()\n",
    "# df['the_preceding_word_given_current_token_w_1'] = df.apply(lambda x: get_the_preceding_word(x, window_size = 1), axis = 1)\n",
    "# df['the_succeeding_word_given_current_token_w_1'] = df.apply(lambda x: get_the_succeeding_word(x, window_size = 1), axis = 1)\n",
    "# df['the_preceding_word_given_current_token_w_2'] = df.apply(lambda x: get_the_preceding_word(x, window_size = 2), axis = 1)\n",
    "# df['the_succeeding_word_given_current_token_w_2'] = df.apply(lambda x: get_the_succeeding_word(x, window_size = 2), axis = 1)\n",
    "# df['the_preceding_word_given_current_token_w_3'] = df.apply(lambda x: get_the_preceding_word(x, window_size = 3), axis = 1)\n",
    "# df['the_succeeding_word_given_current_token_w_3'] = df.apply(lambda x: get_the_succeeding_word(x, window_size = 3), axis = 1)\n",
    "# df['the_preceding_word_given_current_token_w_4'] = df.apply(lambda x: get_the_preceding_word(x, window_size = 4), axis = 1)\n",
    "# df['the_succeeding_word_given_current_token_w_4'] = df.apply(lambda x: get_the_succeeding_word(x, window_size = 4), axis = 1)\n",
    "# # increase the window_size\n",
    "# df['the_preceding_word_given_current_token_w_5'] = df.apply(lambda x: get_the_preceding_word(x, window_size = 5), axis = 1)\n",
    "# df['the_succeeding_word_given_current_token_w_5'] = df.apply(lambda x: get_the_succeeding_word(x, window_size = 5), axis = 1)\n",
    "# df['the_preceding_word_given_current_token_w_6'] = df.apply(lambda x: get_the_preceding_word(x, window_size = 6), axis = 1)\n",
    "# df['the_succeeding_word_given_current_token_w_6'] = df.apply(lambda x: get_the_succeeding_word(x, window_size = 6), axis = 1)\n",
    "# df['the_preceding_word_given_current_token_w_7'] = df.apply(lambda x: get_the_preceding_word(x, window_size = 7), axis = 1)\n",
    "# df['the_succeeding_word_given_current_token_w_7'] = df.apply(lambda x: get_the_succeeding_word(x, window_size = 7), axis = 1)\n",
    "# e = time.time()\n",
    "# print (e-s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w swifter.. slower than wo swifter. It need u to modify ur code for vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import swifter\n",
    "\n",
    "# #--------------------------\n",
    "# # preprocessing for contextual information\n",
    "# #--------------------------\n",
    "# s = time.time()\n",
    "# df['the_preceding_word_given_current_token_w_1'] = df.swifter.apply(lambda x: get_the_preceding_word(x, window_size = 1), axis = 1)\n",
    "# df['the_succeeding_word_given_current_token_w_1'] = df.swifter.apply(lambda x: get_the_succeeding_word(x, window_size = 1), axis = 1)\n",
    "# df['the_preceding_word_given_current_token_w_2'] = df.swifter.apply(lambda x: get_the_preceding_word(x, window_size = 2), axis = 1)\n",
    "# df['the_succeeding_word_given_current_token_w_2'] = df.swifter.apply(lambda x: get_the_succeeding_word(x, window_size = 2), axis = 1)\n",
    "# df['the_preceding_word_given_current_token_w_3'] = df.swifter.apply(lambda x: get_the_preceding_word(x, window_size = 3), axis = 1)\n",
    "# df['the_succeeding_word_given_current_token_w_3'] = df.swifter.apply(lambda x: get_the_succeeding_word(x, window_size = 3), axis = 1)\n",
    "# df['the_preceding_word_given_current_token_w_4'] = df.swifter.apply(lambda x: get_the_preceding_word(x, window_size = 4), axis = 1)\n",
    "# df['the_succeeding_word_given_current_token_w_4'] = df.swifter.apply(lambda x: get_the_succeeding_word(x, window_size = 4), axis = 1)\n",
    "# # increase the window_size\n",
    "# df['the_preceding_word_given_current_token_w_5'] = df.swifter.apply(lambda x: get_the_preceding_word(x, window_size = 5), axis = 1)\n",
    "# df['the_succeeding_word_given_current_token_w_5'] = df.swifter.apply(lambda x: get_the_succeeding_word(x, window_size = 5), axis = 1)\n",
    "# df['the_preceding_word_given_current_token_w_6'] = df.swifter.apply(lambda x: get_the_preceding_word(x, window_size = 6), axis = 1)\n",
    "# df['the_succeeding_word_given_current_token_w_6'] = df.swifter.apply(lambda x: get_the_succeeding_word(x, window_size = 6), axis = 1)\n",
    "# df['the_preceding_word_given_current_token_w_7'] = df.swifter.apply(lambda x: get_the_preceding_word(x, window_size = 7), axis = 1)\n",
    "# df['the_succeeding_word_given_current_token_w_7'] = df.swifter.apply(lambda x: get_the_succeeding_word(x, window_size = 7), axis = 1)\n",
    "# e = time.time()\n",
    "# print (e-s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------\n",
    "# setting \n",
    "#---------------\n",
    "num_partitions = 10\n",
    "cpu_rate = 0.5\n",
    "num_cores = int(multiprocessing.cpu_count() * cpu_rate)\n",
    "num_cores = 10\n",
    "def parallelize_dataframe(df, func):\n",
    "    df1,df2,df3,df4,df5,df6,df7,df8,df9,df10 = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, [df1,df2,df3,df4,df5,df6,df7,df8,df9,df10]))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def speed_up_func_for_preprocessing(df):\n",
    "    '''\n",
    "    Put the columns u need to apply()\n",
    "    \n",
    "    data: DataFrame\n",
    "    '''\n",
    "    df['the_preceding_word_given_current_token_w_1'] = df.apply(lambda x: get_the_preceding_word(x, window_size = 1), axis = 1)\n",
    "    df['the_succeeding_word_given_current_token_w_1'] = df.apply(lambda x: get_the_succeeding_word(x, window_size = 1), axis = 1)\n",
    "    df['the_preceding_word_given_current_token_w_2'] = df.apply(lambda x: get_the_preceding_word(x, window_size = 2), axis = 1)\n",
    "    df['the_succeeding_word_given_current_token_w_2'] = df.apply(lambda x: get_the_succeeding_word(x, window_size = 2), axis = 1)\n",
    "    df['the_preceding_word_given_current_token_w_3'] = df.apply(lambda x: get_the_preceding_word(x, window_size = 3), axis = 1)\n",
    "    df['the_succeeding_word_given_current_token_w_3'] = df.apply(lambda x: get_the_succeeding_word(x, window_size = 3), axis = 1)\n",
    "    df['the_preceding_word_given_current_token_w_4'] = df.apply(lambda x: get_the_preceding_word(x, window_size = 4), axis = 1)\n",
    "    df['the_succeeding_word_given_current_token_w_4'] = df.apply(lambda x: get_the_succeeding_word(x, window_size = 4), axis = 1)\n",
    "    # increase the window_size\n",
    "    df['the_preceding_word_given_current_token_w_5'] = df.apply(lambda x: get_the_preceding_word(x, window_size = 5), axis = 1)\n",
    "    df['the_succeeding_word_given_current_token_w_5'] = df.apply(lambda x: get_the_succeeding_word(x, window_size = 5), axis = 1)\n",
    "    df['the_preceding_word_given_current_token_w_6'] = df.apply(lambda x: get_the_preceding_word(x, window_size = 6), axis = 1)\n",
    "    df['the_succeeding_word_given_current_token_w_6'] = df.apply(lambda x: get_the_succeeding_word(x, window_size = 6), axis = 1)\n",
    "    df['the_preceding_word_given_current_token_w_7'] = df.apply(lambda x: get_the_preceding_word(x, window_size = 7), axis = 1)\n",
    "    df['the_succeeding_word_given_current_token_w_7'] = df.apply(lambda x: get_the_succeeding_word(x, window_size = 7), axis = 1)\n",
    "    return df\n",
    "\n",
    "def speed_up_func_for_feature_engineering(df):\n",
    "    '''\n",
    "    Put the columns u need to apply()\n",
    "    \n",
    "    data: DataFrame\n",
    "    '''\n",
    "    # succeeding_2_gram_given_current_token\n",
    "    df['succeeding_2_gram_given_current_token'] = df.apply(lambda x: succeeding_2_gram_given_current_token(x, esBigramFreq = esBigramFreq), axis = 1) \n",
    "    # preceding_2_gram_given_current_token\n",
    "    df['preceding_2_gram_given_current_token'] = df.apply(lambda x: preceding_2_gram_given_current_token(x, esBigramFreq = esBigramFreq), axis = 1) \n",
    "    # succeeding_3_gram_given_current_token\n",
    "    df['succeeding_3_gram_given_current_token'] = df.apply(lambda x: succeeding_3_gram_given_current_token(x, esTrigramFreq = esTrigramFreq), axis = 1) \n",
    "    # preceding_3_gram_given_current_token\n",
    "    df['preceding_3_gram_given_current_token'] = df.apply(lambda x: preceding_3_gram_given_current_token(x, esTrigramFreq = esTrigramFreq), axis = 1) \n",
    "    # succeeding_4_gram_given_current_token\n",
    "    df['succeeding_4_gram_given_current_token'] = df.apply(lambda x: succeeding_4_gram_given_current_token(x, esFgramFreq = esFgramFreq), axis = 1) \n",
    "    # preceding_4_gram_given_current_token\n",
    "    df['preceding_4_gram_given_current_token'] = df.apply(lambda x: preceding_4_gram_given_current_token(x, esFgramFreq = esFgramFreq), axis = 1) \n",
    "    # succeeding_5_gram_given_current_token\n",
    "    df['succeeding_5_gram_given_current_token'] = df.apply(lambda x: succeeding_5_gram_given_current_token(x, esFivegrams = esFivegrams), axis = 1) \n",
    "    # preceding_5_gram_given_current_token\n",
    "    df['preceding_5_gram_given_current_token'] = df.apply(lambda x: preceding_5_gram_given_current_token(x, esFivegrams = esFivegrams), axis = 1) \n",
    "    # succeeding_strip_2_gram_given_current_token\n",
    "    df['succeeding_strip_2_gram_given_current_token'] = df.apply(lambda x: succeeding_strip_2_gram_given_current_token(x, esBigramFreq = esBigramFreq), axis = 1) \n",
    "    # preceding_strip_2_gram_given_current_token\n",
    "    df['preceding_strip_2_gram_given_current_token'] = df.apply(lambda x: preceding_strip_2_gram_given_current_token(x, esBigramFreq = esBigramFreq), axis = 1) \n",
    "    # succeeding_strip_3_gram_given_current_token\n",
    "    df['succeeding_strip_3_gram_given_current_token'] = df.apply(lambda x: succeeding_strip_3_gram_given_current_token(x, esBigramFreq = esBigramFreq), axis = 1) \n",
    "    # preceding_strip_3_gram_given_current_token\n",
    "    df['preceding_strip_3_gram_given_current_token'] = df.apply(lambda x: preceding_strip_3_gram_given_current_token(x, esBigramFreq = esBigramFreq), axis = 1) \n",
    "    # succeeding_strip_4_gram_given_current_token\n",
    "    df['succeeding_strip_4_gram_given_current_token'] = df.apply(lambda x: succeeding_strip_4_gram_given_current_token(x, esBigramFreq = esBigramFreq), axis = 1) \n",
    "    # preceding_strip_4_gram_given_current_token\n",
    "    df['preceding_strip_4_gram_given_current_token'] = df.apply(lambda x: preceding_strip_4_gram_given_current_token(x, esBigramFreq = esBigramFreq), axis = 1) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9531095027923584\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "df = parallelize_dataframe(df, speed_up_func_for_preprocessing)\n",
    "e = time.time()\n",
    "print (e-s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41443, 18)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "(\"'generator' object is not subscriptable\", 'occurred at index 0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-77-61a4a2be03be>\", line 58, in speed_up_func_for_feature_engineering\n    df['succeeding_5_gram_given_current_token'] = df.apply(lambda x: succeeding_5_gram_given_current_token(x, esFivegrams = esFivegrams), axis = 1)\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/pandas/core/frame.py\", line 6014, in apply\n    return op.get_result()\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/pandas/core/apply.py\", line 142, in get_result\n    return self.apply_standard()\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/pandas/core/apply.py\", line 248, in apply_standard\n    self.apply_series_generator()\n  File \"/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/site-packages/pandas/core/apply.py\", line 277, in apply_series_generator\n    results[i] = self.f(v)\n  File \"<ipython-input-77-61a4a2be03be>\", line 58, in <lambda>\n    df['succeeding_5_gram_given_current_token'] = df.apply(lambda x: succeeding_5_gram_given_current_token(x, esFivegrams = esFivegrams), axis = 1)\n  File \"<ipython-input-69-d2bc0b6e79af>\", line 129, in succeeding_5_gram_given_current_token\n    return esFivegrams[key]\nTypeError: (\"'generator' object is not subscriptable\", 'occurred at index 0')\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-e9123b1c007f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_parell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallelize_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeed_up_func_for_feature_engineering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-77-61a4a2be03be>\u001b[0m in \u001b[0;36mparallelize_dataframe\u001b[0;34m(df, func)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_partitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_cores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/linuxbrew/.linuxbrew/opt/python/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: (\"'generator' object is not subscriptable\", 'occurred at index 0')"
     ]
    }
   ],
   "source": [
    "df_parell = parallelize_dataframe(df, speed_up_func_for_feature_engineering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
